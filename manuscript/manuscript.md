---
title: Automated title and abstract screening for scoping reviews using the GPT-4 Large Language Model
authors: David Wilkins
bibliography: bibliography.bib
---

# Introduction

A scoping review is a relatively novel type of literature review that aims to map the key concepts and existing activity within an area of research [@Arksey.2005]. Like systematic reviews, scoping reviews typically use rigorous, transparent [@Pham.2014], and sometimes pre-registered methods for gathering and synthesising evidence, and increasingly use formal frameworks for both performing and reporting reviews [@Peters.2021]. Scoping reviews can inform future systematic reviews or primary research in the same area [@Sutton.2019]. However, they differ from systematic reviews in aiming to describe the breadth of coverage of the available literature rather than research findings in depth [@Arksey.2005].

Frameworks for performing a scoping review typically involve defining a research area or question, searching bibliographic databases for potentially relevant published material ('sources'), screening these sources to identify those relevant to the area or question, and systematically extracting and reporting data from the relevant sources [@JBI.2015; @Arksey.2005]. The screening stage will usually involve initial screening of source titles and abstracts against pre-determined inclusion and exclusion criteria, followed by screening of source full texts, with both steps performed in replicate by at least two human reviewers [@Peters.2020; @Pham.2014]. Because database searches can return many hundreds or thousands of potentially relevant sources, these screening steps can require intensive human effort and time. Many software methods have been proposed or used to support or partially automate source screening for scoping reviews, including text mining to prioritise potentially more relevant sources for human screening [@Shemilt.2014; @Howard.2016; @Chai.2021], automated clustering and labelling of sources to support human decision-making [@Stansfield.2013], and 'crowdsourcing' screening to untrained workers via online platforms [@Mortensen.2017]. A similar but more extensive set of methods have been developed and employed for systematic reviews [@Khalil.2022; @Gates.2019] for which the process of source screening is broadly comparable.

Since the release of the first Generative Pre-trained Transformer (GPT) Large Language Model (LLM) by OpenAI (San Francisco, California, United States of America) in 2018 [@Radford.2018], transformer-based LLMs and the GPT lineage in particular have seen rapid and widespread adoption for a range of automation tasks. Broadly, these models generate a probabilistically weighted list of `tokens' (parts of text such as letter combinations and punctuation) which may continue or complete some input text (a 'prompt'), having been trained to do so by practising such predictions against large human-written corpora. When this generative process is iterated, it allows for a range of applications involving analysis and production of text, such as summarising documents, generating fiction in a particular genre or style, or conversing with humans [@OpenAI.2023].

While LLMs are not yet widely used to screen sources for literature reviews, early work suggests they may perform well in this role. Guo et al. [@Guo.2023] reported the use of a GPT-lineage model (they do not specify which, though their published code suggests OpenAI's 'gpt-3.5-turbo' model) to screen 24,307 titles and abstracts from five systematic reviews and one scoping review, achieving weighted average sensitivity of 76% and specificity of 91% when compared to human reviewers. Their approach involves giving the model a brief prompt instructing it to take on the persona of a researcher screening titles and abstracts, followed by a source's title and abstract as well as the inclusion and exclusion criteria. The model is instructed to respond with a decision to include or exclude the source, and the process is iterated across the full set of sources to be screened. Syriani et al. [@Syriani.2023] similarly reported the use of 'gpt-3.5-turbo' to screen titles and abstracts for a systematic review and achieved sensitivities of above 70%. They also systematically evaluated prompts given to the LLM to identify a prompt that performed best at the screening task; their chosen prompt, like that of Guo et al., placed the LLM in the role of an academic reviewer.

Both of these approaches made use of a single, fixed text prompt template, which the LLM then completes with additional text representing its response (the decision to include or exclude a source), a method sometimes called 'zero-shot prompting'. Recent work has identified a number of methods which can be superior to zero-shot prompting when using LLMs for tasks that require complex or multi-step reasoning. These methods include 'chain-of-thought prompting' [@Wei.2022], in which a complex task is broken down into a series of intermediate steps, and the 'tree of thoughts' strategy [@Yao.2023], in which multiple parallel chains of thought are generated, compared, and integrated. The LLM is induced to follow these complex reasoning strategies either by being given examples of multi-step reasoning on similar tasks, or by being lead through the process with a series of intermediate prompts.

In this paper, I introduce a package for the R programming language [@R.2023] called GPTscreenR that implements a chain-of-thought based approach to using GPT-4 for scoping review title and abstract screening, and evaluate its performance by comparison to human reviewers. The purpose of this package is to assist and augment rather than replace human reviewers in performing scoping reviews. This package represents a novel development in the use of LLMs for source screening in literature reviews, as it provides an open-source and immediately available screening tool which can be downloaded, used, and modified by academic reviewers, and is the first LLM-based screening tool designed specifically for scoping reviews. Further, this manuscript provides the first report on the accuracy of LLM-based screening using the most recent iteration of the GPT model lineage, GPT-4, and using the recently developed chain-of-thought approach to maximise screening accuracy.

# Methods

## The GPTscreenR package

GPTscreenR is an R [@R.2023] package released under the MIT open source licence. The source code is available for download from GitHub at \url{https://github.com/wilkox/GPTscreenR}. At the time of writing the most recent package version was 0.0.1; the results presented in this paper were obtained with an earlier version 0.0.0.9005, since which small changes have been made to further refine performance.

GPTscreenR consists of two main components. The first is a set of internal functions for interfacing with the OpenAI Application Programming Interface (API), which allows for `conversations' with the GPT-4 LLM, as well as functions for representing and manipulating those conversations. These internal functions are designed to be model-agnostic, so that future versions of the package or users with particular needs can use different GPT models. The OpenAI API requires an OpenAI account, and OpenAI charges fees for use of the API. In order to access the API, GPTscreenR requires a secret key to be registered prior to source screening, and instructions for doing so are provided in the package documentation and on loading of the package in R if the key has not been correctly registered.

The second component is a set of user-facing functions to perform source screening with GPT-4. The `review_description()` function assists in generating a text description of the review's objectives and inclusion and exclusion criteria, using the Population, Concept, and Context (PCC) frame [@Peters.2020] for defining the review's inclusion and exclusion criteria. The use of this function is optional, and users may instead choose to provide a description of the review and criteria for source selection using any framework or format they see fit.

The `screen_source()` function performs the main task of the package. This function mediates an conversation with GPT-4 in which chain-of-thought prompting [@Wei.2022] is used to guide GPT-4 through screening a source title and abstract against the study inclusion criteria. The template for this conversation is given in Fig. 1. The OpenAI API defines a conversation as a series of messages, each of which originates from one of three roles: \textit{system}, representing an authoritative voice that can instruct GPT-4 on its task and behaviour; \textit{user}, representing a human user that can interact with GPT-4; and \textit{assistant}, representing the responses generated by GPT-4. In `screen_source()`, the \textit{system} role gives GPT-4 general instructions, while the \textit{user} role provides the user-written review description and the source title and abstract.

![Template for the conversation with GPT-4, mediated by the `GPTscreenR` package. Messages with variable content, including user-provided data as well as GPT-4's responses, are given in italics. The typographic error in the first message ('search search') appeared in the version of the package used for the results reported in this manuscript, but has been since corrected.](./fig_1.pdf)

The phrase 'work step by step', used each time \textit{system} gives GPT-4 a prompt with specific instructions, is derived from the 'Let's think step by step' prompt phrase which significantly improves LLM performance on multi-step reasoning tasks with a zero-shot prompt [@Kojima.2022], adapted to this chain-of-thought approach. This approach thus attempts to maximise engagement of GPT-4's multi-step reasoning capabilities both across and within each step in the screening task.

GPT-4 is serially instructed to summarise the inclusion criteria for the scoping review (prior to being presented with the title and abstract to be screened), compare the title and abstract against these summarised criteria, and make a final recommendation on whether to include or exclude the source (Fig. 1). Following the chain-of-thought approach, an example of summarised inclusion criteria is given for GPT-4 to use as a template or exemplar. This approach was chosen after noting that a major source of type II error (false positives) when attempting to screen sources with zero-shot, one-shot, or few-shot prompts (i.e. with a single prompt and no, one, or a few examples) was that GPT-4 would fail to consider important inclusion criteria. As an example, Fig 2a presents a conversation with GPT-4 using a zero-shot prompt. The screening task in this example is intentionally adversarial and designed to lead the model towards making an error. In order to correctly recommend exclusion of the source, GPT-4 must notice that the review is looking for research on therapy alpaca interventions, but that the source reports on a therapy camel intervention. The presence of multiple other inclusion criteria which are met by the source, as well as the mention of alpacas in the source abstract, serve as distractors. In this example, GPT-4 incorrectly recommends inclusion. If the conversation is then continued to draw GPT-4's attention to the error, it is able to identify and correct it (Fig 2b), suggesting that the error arises from a failure of GPT-4 to properly consider the relevant inclusion criterion rather than an inability to do so. Using the chain-of-thought approach overcomes this problem without the need for human intervention and correction (Fig 2c). GPT-4 identifies 'The source examines the impact of therapy alpaca programmes' as an inclusion criterion, and correctly assesses that this is the only inclusion criterion not met. GPT-4 then correctly recommends exclusion of the source.

\setcounter{figure}{0}
\renewcommand\thefigure{2\alph{figure}}

![A conversation with GPT-4, using a zero-shot prompt to instruct the model to screen a single source for a hypothetical scoping review. This adversarial task has been intentionally designed to influence GPT-4 towards making an error.](./fig_2a.pdf)

![A continuation of the conversation in Fig. 2a, in which GPT-4 is lead to recognise and correct the error.](./fig_2b.pdf)

![A conversation with GPT-4, instructing it to perform the same adversarial task presented in Fig. 2a, but using the chain-of-thought approach. This is not a continuation of the conversation presented in Fig. 2a and Fig. 2b but a new conversation. In this case, GPT-4 correctly recommends excluding the source. The model also correctly identifies that the source meets all of the relevant inclusion criteria except for the requirement that the source report on an alpaca intervention.](./fig_2c.pdf)

The `screen_source()` function returns a list comprising the complete transcript of the conversation with GPT-4 and GPT-4's final recommendation, either the word 'INCLUDE' or 'EXCLUDE'. The conversation transcript can be used to interrogate cases where GPT-4 may have returned an incorrect or unexpected result. The package also provides a function `screen_sources()`, which applies `screen_source()` iteratively across a data frame of sources. `screen_sources()` caches screening outcomes to a file as they accumulate, so that screening can be split across multiple sessions and recover from interruptions.

## Validation

To validate GPTscreenR's approach, six scoping reviews were identified from the Open Science Framework (OSF; \url{https://osf.io}) where the review inclusion criteria and the results of title and abstract screening were publicly available. A summary of the review characteristics is provided in Table 1. Small random subsets of screened sources from four of the reviews (`COVID`, `smartphones`, `solastalgia`, and `teachers`) were used during initial testing and refinement of the `screen_source()` function, while the full set or random subsets from all six reviews were used for final validation. Random subsets were used where the large number of sources made it prohibitive in time and cost to screen the full set. The total number of sources available for screening and the number used for validation from each review are given in Table 1.

Some of the reviews did not include the full abstract text of sources in the publicly available files, and where these abstracts could not be obtained from external databases these sources were excluded from validation. There were also many cases where missing, malformed, or duplicate data required either manual correction or exclusion of sources. The scoping review data, code used to prepare this data for validation, and code for calculating summary statistics are available in a reproducible form in the package repository on GitHub (\url{https://github.com/wilkox/GPTscreenR/tree/master/validation}).

The final human reviewer decision at the title and abstract screening level was used as the gold standard outcome. Sensitivity and specificity were calculated by comparing GPT-4's recommendation against this gold standard. Three of the scoping reviews (`COVID`, `solastalgia`, and `teachers`) included individual human reviewer decisions in addition to the final decision in their publicly available datasets, and these were used to calculate human intraobserver agreement (Cohen's kappa) using the R function `cohen.kappa()` from the `psych` package [@Revelle.2023]. This was compared to human/GPT-4 agreement across all screened sources, calculated with the same method.

## Comparison to zero-shot method

In order to directly compare the chain-of-thoughts approach to a zero-shot approach (i.e. an conversation consisting of a single prompt with no examples, followed by GPT-4's response), the validation screening task was repeated using the prompt designed by Guo et al. [@Guo.2023], substituting the permitted responses `INCLUDE` and `EXCLUDE` for `included` and `excluded` respectively in order to maintain compatibility with GPTscreenR's parsing of the response (Fig. 3). The number of sources used for zero-shot validation were the same as those used for the chain-of-thoughts method validation given in Table 2, although a different subset was selected by the randomisation process in cases where the full set was not used. The code used to prepare this data and calculate summary statistics was otherwise identical to that used for the chain-of-thoughts method validation and is available in the 'zeroshot' branch of the package repository on GitHub (\url{https://github.com/wilkox/GPTscreenR/tree/zeroshot}).

\setcounter{figure}{2}
\renewcommand\thefigure{\arabic{figure}}

![Zero-shot prompt used as a comparator for the chain-of-thoughts approach, derived from the approach of Guo et al. [@Guo.2023]. Variable content, including user-provided data as well as GPT-4's response, is given in italics.](./fig_3.pdf)

# Results

1,138 sources were screened from the six scoping reviews. GPTscreenR achieved a weighted average sensitivity of 0.75 and weighted average specificity of 0.87. For the three reviews that provided individual reviewer decisions, the weighted average Cohen's kappa was 0.67, while the weighted average Cohen's kappa between human and GPT-4 decisions was 0.53.

The zero-shot method achieved a weighted average sensitivity of 0.58 and weighted average specificity of 0.91. The weighted average Cohen's kappa between human and GPT-4 decisions was 0.52.

# References

::: {#refs}
:::
